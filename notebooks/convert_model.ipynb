{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c195330-2c25-48a6-902c-eec7230b876a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")  # Try \"high\" as well\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import src\n",
    "except:\n",
    "    sys.path.append('../')\n",
    "    import src\n",
    "\n",
    "from src.model import WallModel\n",
    "from src import config\n",
    "from src.transform import get_train_augmentations\n",
    "\n",
    "import coremltools as ct\n",
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634621a-daff-4131-8634-45e7e6f3af2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '../DeepLabV3Plus-mobileone_s3.ckpt'\n",
    "\n",
    "model = WallModel.load_from_checkpoint(\n",
    "    CHECKPOINT_PATH, \n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    init_datasets=False\n",
    ").cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e693a58-1d7f-4a0a-9eb3-a4fbf4b8d65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_model_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7824c-0f8f-4393-9cbd-f81094f31b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reparameterize_model(model):\n",
    "    return smp.encoders.mobileone.reparameterize_model(model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bd354-20fc-4431-93d7-c2f6a0359a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_params_before_reparameterization = count_model_parameters(model)\n",
    "\n",
    "model_reparameterized = reparameterize_model(model)\n",
    "\n",
    "num_params_after_reparameterization = count_model_parameters(model_reparameterized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797de48-6a4e-4604-8ca0-5398b9df902b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of parameters')\n",
    "print(f'Before reparameterization: {num_params_before_reparameterization}')\n",
    "print(f'After reparameterization: {num_params_after_reparameterization}')\n",
    "diff_percentage = float(num_params_after_reparameterization) / float(num_params_before_reparameterization)\n",
    "print(f'Reparameterized model is {diff_percentage:.2%} size of the original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bcea1-015c-49ae-b431-b09e928a64b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WrappedWallModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super(WrappedWallModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x) # NCHW: (batch_size, 1, height, width)\n",
    "        # Apply sigmoid to logits to get actual class\n",
    "        out = out.sigmoid()\n",
    "        # Remove batch and label dimensions\n",
    "        out = out.squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc3307-599f-4a20-b076-60f6f2201725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_wrapped = WrappedWallModel(model_reparameterized.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dfd73-0f1a-4324-b8d4-1a007b1f4490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = torch.rand((1, 3, config.INPUT_IMAGE_SIZE[1], config.INPUT_IMAGE_SIZE[0])) # NCHW\n",
    "print(f'Sample input shape: {sample_input.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42878674-88de-43e8-bd79-98931ee0b952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_params = smp.encoders.get_preprocessing_params(config.ENCODER)\n",
    "print(f'{config.ENCODER} encoder params: {encoder_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f518748-7750-4247-9ec0-768614df77c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample_output = model_wrapped(sample_input)\n",
    "print(f'Sample output shape: {sample_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3329069-ae06-47a1-a0c3-0aa858fe9eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace = torch.jit.trace(model_wrapped, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd61e2-60d0-4b75-a947-79212d79335f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CoreML preprocessing equation: normalized = data * scale + bias\n",
    "# y_red_channel = x_red_channel * scale + red_bias\n",
    "# y_green_channel = x_green_channel * scale + green_bias\n",
    "# y_blue_channel = x_blue_channel * scale + blue_bias\n",
    "# 0.226 = (0.229 + 0.224 + 0.225)/3\n",
    "# We could, however, add this scaling manually by editing model spec directly,\n",
    "# as descrbed in \"Core ML Survival Guide\" on pages 271-275\n",
    "# scale = 1 / (0.226 * 255.0)\n",
    "scale = 1 / (np.mean(encoder_params['std']) * 255.0)\n",
    "# bias = [- 0.485 / (0.229), - 0.456 / (0.224), - 0.406 / (0.225)]\n",
    "bias = [\n",
    "    - encoder_params['mean'][0] / encoder_params['std'][0],\n",
    "    - encoder_params['mean'][1] / encoder_params['std'][1],\n",
    "    - encoder_params['mean'][2] / encoder_params['std'][2],\n",
    "]\n",
    "\n",
    "image_input = ct.ImageType(\n",
    "    name=\"input\",\n",
    "    shape=sample_input.shape,\n",
    "    color_layout=ct.colorlayout.RGB,\n",
    "    scale=scale,\n",
    "    bias=bias\n",
    ")\n",
    "\n",
    "mlmodel = ct.convert(\n",
    "    trace,\n",
    "    inputs=[image_input],\n",
    "    # convert_to=\"mlprogram\",\n",
    "    # minimum_deployment_target=ct.target.iOS16,\n",
    "    # compute_precision=ct.precision.FLOAT16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f0729-85e5-4116-9c70-2c64d4c7ba2d",
   "metadata": {},
   "source": [
    "### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a691ff-5e85-4e4c-82cd-1f148d0607e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_image_path = './apartments.jpeg'\n",
    "image = PIL.Image.open(sample_image_path)\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a6724-7167-4074-ae56-50d609d1e7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = np.asarray(image)\n",
    "plt.imshow(image);\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cbae8-7edd-4998-8ad6-df6f45351fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_preprocessed = get_train_augmentations(augment=False)(image=image)['image']\n",
    "plt.imshow(image_preprocessed)\n",
    "image_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda3e97-403b-4af7-9461-0b15d3a92a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec = mlmodel.get_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452a320-c77e-43f8-ba3a-ab160fd8ce63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_width = spec.description.input[0].type.imageType.width\n",
    "img_height = spec.description.input[0].type.imageType.height\n",
    "img_width, img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96187289-4301-4e59-972d-8d92cc376aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_name = spec.description.input[0].name\n",
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef61f1-8fa2-490a-aabf-7191aff8ce28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = mlmodel.predict({input_name: image_preprocessed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd11708-6d46-422a-8301-858284a13bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set metadata\n",
    "mlmodel.short_description = 'Wall segmentation model'\n",
    "mlmodel.input_description['input'] = 'Input RGB image'\n",
    "\n",
    "output = mlmodel._spec.description.output[0]\n",
    "output.type.multiArrayType.shape.append(config.INPUT_IMAGE_SIZE[1])\n",
    "output.type.multiArrayType.shape.append(config.INPUT_IMAGE_SIZE[0])\n",
    "\n",
    "ct.utils.rename_feature(\n",
    "    mlmodel._spec, \n",
    "    mlmodel.get_spec().description.output[0].name, \n",
    "    \"output\", \n",
    "    rename_inputs=True\n",
    ")\n",
    "\n",
    "mlmodel.output_description['output'] = \"Wall segmentation map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ed392-cd0f-47ec-a069-e8de9dc28164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_filename = f'{config.ARCHITECTURE}-{config.ENCODER}.mlpackage'\n",
    "mlmodel.save(model_filename)\n",
    "print(f'MLModel saved to {model_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c259d-7aa7-4a56-8617-94574b0f0167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!zip -r ./DeepLabV3Plus-mobileone_s3.mlpackage.zip ./DeepLabV3Plus-mobileone_s3.mlpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89434959-1d6a-4137-8f81-a9f0d56a0efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.sigmoid().squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb32873-2347-467e-954f-1af54b27055c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
